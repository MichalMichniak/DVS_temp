{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardTanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=-1.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(5, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same'),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(5, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer0): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "      )\n",
       "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): MaxPool2d(kernel_size=7, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = ResNet(ResidualBlock, [3, 4, 6, 3], num_classes = 10).to(\"cpu\")\n",
    "model2.load_state_dict(torch.load(\"best_resnet50_MINST-DVS_Hardtanh_ReLUmaxpool.pt\", weights_only=True))\n",
    "model2.to(\"cpu\")\n",
    "model2.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na ISNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_spiking(tj, W, D_i, t_min, t_max, noise, dtype=torch.FloatTensor):\n",
    "    \"\"\"\n",
    "    Calculates spiking times to recover ReLU-like functionality.\n",
    "    Assumes tau_c=1 and B_i^(n)=1.\n",
    "    \"\"\"\n",
    "    # Calculate the spiking threshold (Eq. 18)\n",
    "    threshold = t_max - t_min - D_i\n",
    "    \n",
    "    # Calculate output spiking time ti (Eq. 7)\n",
    "\n",
    "    ti = torch.matmul((tj - t_min).type(dtype), W.type(dtype)) + threshold + t_min\n",
    "    \n",
    "    # Ensure valid spiking time: do not spike for ti >= t_max\n",
    "    ti = torch.where(ti < t_max, ti, t_max)\n",
    "\n",
    "    # Add noise to the spiking time for noise simulations\n",
    "    if noise > 0:\n",
    "        ti = ti + torch.randn_like(ti) * noise\n",
    "    \n",
    "    return ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingDense(nn.Module):\n",
    "    def __init__(self, units, name, X_n=1, outputLayer=False, robustness_params={}, input_dim=None,\n",
    "                 kernel_regularizer=None, kernel_initializer=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.B_n = (1 + 0.0) * X_n\n",
    "        self.outputLayer=outputLayer\n",
    "        self.t_min_prev, self.t_min, self.t_max=0, 0, 1\n",
    "        self.noise=robustness_params['noise']\n",
    "        self.time_bits=robustness_params['time_bits']\n",
    "        self.weight_bits =robustness_params['weight_bits'] \n",
    "        self.w_min, self.w_max=-1.0, 1.0\n",
    "        self.alpha = torch.full((units,), 1, dtype=torch.float64)\n",
    "        self.input_dim=input_dim\n",
    "        self.regularizer = kernel_regularizer\n",
    "        self.initializer = kernel_initializer\n",
    "        self.bias = False\n",
    "    \n",
    "    def build(self, input_dim, kernel : torch.Tensor = None, bias : torch.Tensor = None):\n",
    "        # Ensure input_dim is defined properly if not passed.\n",
    "        if input_dim[-1] is None:\n",
    "            input_dim = (None, self.input_dim)\n",
    "        else:\n",
    "            self.input_dim = input_dim\n",
    "        # Create kernel weights and D_i.\n",
    "        if kernel is not None:\n",
    "            if bias is None:\n",
    "                self.kernel = nn.Parameter(kernel.clone())\n",
    "            else:\n",
    "                self.kernel = nn.Parameter(torch.concat((kernel.clone(),bias.clone().unsqueeze(0))))\n",
    "                self.bias = True\n",
    "        else:\n",
    "            self.kernel = nn.Parameter(torch.empty(input_dim[-1], self.units))\n",
    "        self.D_i = nn.Parameter(torch.zeros(self.units))\n",
    "\n",
    "        # Apply the initializer if provided.\n",
    "        if self.initializer:\n",
    "            self.kernel = self.initializer(self.kernel) # tu zmiana TODO\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max parameters of this layer. Alpha is fixed at 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias:\n",
    "            max_W = torch.concat((torch.maximum(self.kernel[:-1],torch.zeros(self.kernel[:-1].shape)), self.kernel[-1].unsqueeze(0)))\n",
    "            max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
    "        else:\n",
    "            max_input = torch.tensor(in_ranges_max)\n",
    "            max_W = torch.maximum(self.kernel,torch.zeros(self.kernel.shape))\n",
    "        output_val = F.relu(torch.matmul(max_input,max_W))\n",
    "\n",
    "        if self.bias:\n",
    "            max_W = torch.concat((torch.maximum(self.kernel[:-1],torch.zeros(self.kernel[:-1].shape)), torch.maximum(self.kernel[-1].unsqueeze(0), torch.zeros(self.kernel[-1].unsqueeze(0).shape))))\n",
    "            max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
    "        else:\n",
    "            max_input = torch.tensor(in_ranges_max)\n",
    "            max_W = torch.maximum(self.kernel,torch.zeros(self.kernel.shape))\n",
    "        max_V = F.relu(torch.max(torch.matmul(max_input,max_W)))\n",
    "\n",
    "        self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "        \n",
    "        # Returning for function signature consistency\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), output_val\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times `tj`, output spiking times `ti` or membrane potential value for the output layer.\n",
    "        \"\"\"\n",
    "        # Call the custom spiking logic\n",
    "        if self.bias:\n",
    "            print(tj.shape)\n",
    "            new_tj = torch.concat((tj, torch.tensor([[(self.t_min - 1)]])), dim=1)\n",
    "            output = call_spiking(new_tj, self.kernel, self.D_i, self.t_min, self.t_max, noise=self.noise)\n",
    "        else:\n",
    "            output = call_spiking(tj, self.kernel, self.D_i, self.t_min, self.t_max, noise=self.noise)\n",
    "        # If this is the output layer, perform the special integration logic\n",
    "        if self.outputLayer:\n",
    "            # Compute weighted product\n",
    "            W_mult_x = torch.matmul(self.t_min - tj, self.kernel)\n",
    "            self.alpha = self.D_i / (self.t_min - self.t_min_prev)\n",
    "            output = self.alpha * (self.t_min - self.t_min_prev) + W_mult_x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingConv2D(nn.Module):\n",
    "    def __init__(self, filters, name, X_n=1, padding='same', kernel_size=(3,3), robustness_params=None, kernels = None, device = 'cuda:0', biases = None, stride=1):\n",
    "        super(SpikingConv2D, self).__init__()\n",
    "        self.stride = stride\n",
    "        if robustness_params is None:\n",
    "            robustness_params = {}\n",
    "        \n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.B_n = (1 + 0.0) * X_n\n",
    "        self.t_min_prev, self.t_min, self.t_max = 0, 0, 1\n",
    "        self.w_min, self.w_max = -1.0, 1.0\n",
    "        self.time_bits = robustness_params.get('time_bits', 1)\n",
    "        self.weight_bits = robustness_params.get('weight_bits', 1) \n",
    "        self.noise = robustness_params.get('noise', 0.0)\n",
    "        self.device = device\n",
    "        # Initialize alpha as a tensor of ones\n",
    "        self.alpha = nn.Parameter(torch.ones(filters, dtype=torch.float32))\n",
    "        \n",
    "        # Registering the kernel as a learnable parameter\n",
    "        #TODO:\n",
    "        if kernels is not None:\n",
    "            self.kernel = nn.Parameter(kernels).to(device)\n",
    "        else:\n",
    "            self.kernel = nn.Parameter(torch.randn(filters, 1, kernel_size[0], kernel_size[1], dtype=torch.float32)).to(device)\n",
    "        if biases is not None:\n",
    "            self.B = biases.unsqueeze(1).to(self.device)\n",
    "        else:\n",
    "            self.B = nn.Parameter(torch.zeros(filters, 1, dtype=torch.float32)).to(self.device)\n",
    "\n",
    "        # Placeholder for batch normalization parameters\n",
    "        self.BN = nn.Parameter(torch.tensor([0], dtype=torch.float32), requires_grad=False)\n",
    "        self.BN_before_ReLU = nn.Parameter(torch.tensor([0], dtype=torch.float32), requires_grad=False)\n",
    "        \n",
    "        # Parameter for different thresholds\n",
    "        self.D_i = nn.Parameter(torch.zeros(9, filters, dtype=torch.float32)).to(self.device)\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        max_W = torch.maximum(self.kernel,torch.zeros(self.kernel.shape).to(self.device))\n",
    "        # print(max_W.shape)\n",
    "        \n",
    "        max_input = (in_ranges_max.unsqueeze(-1).unsqueeze(-1)).to(self.device) * torch.ones(self.kernel.shape[1:]).to(self.device)\n",
    "\n",
    "        # print(max_input.shape)\n",
    "        if self.B is not None:\n",
    "            max_V = F.relu(torch.max(torch.sum(torch.mul(max_input,max_W),(1,2,3))+self.B.squeeze(1)))\n",
    "            max_values = F.relu(torch.sum(torch.mul(max_input,max_W),(1,2,3))+self.B.squeeze(1))\n",
    "            max_V = F.relu(torch.max(torch.sum(torch.mul(max_input,max_W),(1,2,3))+torch.maximum(self.B.squeeze(1), torch.zeros(self.B.squeeze(1).shape))))\n",
    "        else:\n",
    "            max_V = F.relu(torch.max(torch.sum(torch.mul(max_input,max_W),(1,2,3))))\n",
    "            max_values = F.relu(torch.sum(torch.mul(max_input,max_W),(1,2,3)))\n",
    "        self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        \n",
    "        # Returning for function signature consistency\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), max_values\n",
    "\n",
    "    def call_spiking(self, tj, W, D_i, t_min, t_max, noise):\n",
    "        \"\"\"\n",
    "        Calculates spiking times from which ReLU functionality can be recovered.\n",
    "        \"\"\"\n",
    "        threshold = t_max - t_min - D_i\n",
    "        \n",
    "        # Calculate output spiking time ti\n",
    "        ti = torch.matmul(tj - t_min, W) + threshold + t_min\n",
    "        \n",
    "        # Ensure valid spiking time\n",
    "        ti = torch.where(ti < t_max, ti, t_max)\n",
    "        \n",
    "        # Add noise\n",
    "        if noise > 0:\n",
    "            ti += torch.randn_like(ti) * noise\n",
    "        \n",
    "        return ti\n",
    "\n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times tj, output spiking times ti. \n",
    "        \"\"\"\n",
    "        if self.stride==1:\n",
    "            padding_size = int(self.padding == 'same') * ((self.kernel_size[0]-1) // 2)\n",
    "        else:\n",
    "            # dont know if it works with stride other than 1 always set padding to valid\n",
    "            padding_size = int(self.padding == 'same') * ((self.kernel_size[0]-1) // 2)\n",
    "        image_same_size = tj.size(2) \n",
    "        image_valid_size = image_same_size - self.kernel_size[0] + 1\n",
    "\n",
    "\n",
    "        tj_shape = tj.shape\n",
    "        # Dodanie paddingu\n",
    "        if self.padding == 'same':\n",
    "            tj = torch.nn.functional.pad(tj, (padding_size, padding_size, padding_size, padding_size), value=self.t_min)\n",
    "        elif type(self.padding) is tuple:\n",
    "            tj = torch.nn.functional.pad(tj, (self.padding[0], self.padding[0], self.padding[1], self.padding[1]), value=self.t_min)\n",
    "            pass\n",
    "        # Wyciąganie patchy\n",
    "        if self.stride==1:\n",
    "            batch_size, in_channels, input_height, input_width = tj.shape\n",
    "            tj = torch.nn.functional.unfold(tj, kernel_size=self.kernel_size, stride=1).transpose(1, 2)\n",
    "            # Reshape dla wag\n",
    "            W = self.kernel.view(self.filters, -1).t()\n",
    "            out_channels, _, kernel_height, kernel_width = self.kernel.shape\n",
    "            output_height = (input_height - kernel_height) // self.stride + 1\n",
    "            output_width = (input_width - kernel_width) // self.stride + 1\n",
    "        else:\n",
    "            batch_size, in_channels, input_height, input_width = tj.shape\n",
    "            tj = torch.nn.functional.unfold(tj, kernel_size=self.kernel_size, stride=self.stride).transpose(1, 2)\n",
    "            out_channels, _, kernel_height, kernel_width = self.kernel.shape\n",
    "            output_height = (input_height - kernel_height) // self.stride + 1\n",
    "            output_width = (input_width - kernel_width) // self.stride + 1\n",
    "            # Reshape dla wag\n",
    "            W = self.kernel.view(out_channels, -1).t()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (self.padding == 'valid' or self.BN != 1 or self.BN_before_ReLU == 1) and (self.B is None): \n",
    "\n",
    "            ti = self.call_spiking(tj, W, self.D_i[0], self.t_min, self.t_max, noise=self.noise).transpose(1, 2)\n",
    "            if self.padding == 'valid':\n",
    "                ti = ti.view(batch_size, out_channels, output_height, output_width)\n",
    "            else:\n",
    "                ti = ti.view(batch_size, out_channels, output_height, output_width)\n",
    "\n",
    "        elif self.B is not None:\n",
    "            ## concatenating simple \"one\" to vector of times\n",
    "            one_as_time = self.t_min - 1\n",
    "            tj = torch.concat((tj, one_as_time * torch.ones(tj.shape[0],tj.shape[1],1).to(self.device)), 2)\n",
    "            ## conttenating biases to weight vector\n",
    "            W = torch.concat((W,self.B.T),0)\n",
    "            ti = self.call_spiking(tj, W, self.D_i[0], self.t_min, self.t_max, noise=self.noise).transpose(1, 2)\n",
    "            if self.padding == 'valid':\n",
    "                ti = ti.view(batch_size, out_channels, output_height, output_width)\n",
    "            else:\n",
    "                ti = ti.view(batch_size, out_channels, output_height, output_width)\n",
    "\n",
    "        return ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_and_bn(conv, bn, device = 'cuda:0'):\n",
    "\t#\n",
    "\t# init\n",
    "\tfusedconv = torch.nn.Conv2d(\n",
    "\t\tconv.in_channels,\n",
    "\t\tconv.out_channels,\n",
    "\t\tkernel_size=conv.kernel_size,\n",
    "\t\tstride=conv.stride,\n",
    "\t\tpadding=conv.padding,\n",
    "\t\tbias=True\n",
    "\t)\n",
    "\t#\n",
    "\t# prepare filters\n",
    "\tw_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "\tw_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))\n",
    "\twith torch.no_grad():\n",
    "\t\tfusedconv.weight.copy_( torch.mm(w_bn, w_conv).view(fusedconv.weight.size()) )\n",
    "\t#\n",
    "\t# prepare spatial bias\n",
    "\tif conv.bias is not None:\n",
    "\t\tb_conv = conv.bias\n",
    "\telse:\n",
    "\t\tb_conv = torch.zeros( conv.weight.size(0) ).to(device)\n",
    "\tb_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "\twith torch.no_grad():\n",
    "\t\tfusedconv.bias.copy_( (torch.matmul(w_bn, b_conv) + b_bn) )\n",
    "\t\n",
    "\treturn fusedconv.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaxMinPool2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Max Pooling or Min Pooling operation, depending on the sign of the batch normalization layer before.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, max_time, stride=None, padding=0, dilation=1):\n",
    "        super(MaxMinPool2D, self).__init__()\n",
    "        \n",
    "        # Default sign is 1, indicating max pooling functionality.\n",
    "        self.sign = nn.Parameter(-1*torch.ones(1, 1, 1, 1), requires_grad=False)\n",
    "        self.dilation = dilation\n",
    "        # MaxPool2d setup (will be used in call)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.max_time = max_time\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Applying the sign to the inputs (if sign is -1, it will act as Min Pooling)\n",
    "        padding_size = self.padding\n",
    "        inputs = torch.nn.functional.pad(inputs, (padding_size, padding_size, padding_size, padding_size), value=self.max_time)\n",
    "        pooled = F.max_pool2d(self.sign * inputs, kernel_size=self.kernel_size, stride=self.stride, padding=0, dilation=self.dilation)\n",
    "        \n",
    "        # Multiply the pooled result by the sign, which controls the pooling type\n",
    "        return pooled * self.sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddSNNLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AddSNNLayer, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input1_val, input2_val, minimal_t_max = 0):\n",
    "        output_val = input1_val + input2_val\n",
    "        max_V = max(output_val)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), output_val\n",
    "\n",
    "    def forward(self, tj1, tj2):\n",
    "        D_i = 0\n",
    "        threshold = self.t_max - self.t_min - D_i\n",
    "        \n",
    "        ti = tj1 + tj2 - 2*self.t_min  + threshold + self.t_min\n",
    "\n",
    "        ti = torch.where(ti < self.t_max, ti, self.t_max)\n",
    "\n",
    "        if self.noise > 0:\n",
    "            ti += torch.randn_like(ti) * self.noise\n",
    "        \n",
    "        return ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSNNLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubSNNLayer, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input1_val, input2_val, minimal_t_max = 0):\n",
    "        self.input1_val = input1_val\n",
    "        output_val = input1_val\n",
    "        max_V = max(output_val)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), output_val\n",
    "\n",
    "    def forward(self, tj1, tj2):\n",
    "        D_i = 0\n",
    "        threshold = self.t_max - self.t_min - D_i\n",
    "        ### Check ###\n",
    "        if((tj2>self.t_min).any() or (tj1>self.t_min).any()):\n",
    "            print(\"XDDD1\")\n",
    "        print(tj1.shape)\n",
    "        if(len(tj1.shape) == 3):\n",
    "            if((torch.amax(self.t_min - tj1, dim=(1, 2))>=(self.input1_val+0.00005)).any()):\n",
    "                print(\"XDDD2\")\n",
    "                print(f\"XDDD2 {(torch.amax(self.t_min - tj1, dim=(1, 2))-self.input1_val).max()}\")\n",
    "                print(self.t_min)\n",
    "                print(torch.amax(self.t_min - tj1, dim=(1, 2)))\n",
    "                print(tj1)\n",
    "        elif(len(tj1.shape) == 4):\n",
    "            if((torch.amax(self.t_min - tj1, dim=(2, 3))>=(self.input1_val+0.00005)).any()):\n",
    "                print(\"XDDD2\")\n",
    "                print(f\"XDDD2 {(torch.amax(self.t_min - tj1, dim=(2, 3))-self.input1_val).max()}\")\n",
    "                print(self.t_min)\n",
    "                print(torch.amax(self.t_min - tj1, dim=(2, 3)))\n",
    "                print(tj1)\n",
    "        elif(len(tj1.shape) == 2):\n",
    "            if((torch.amax(self.t_min - tj1, dim=(1))>=(self.input1_val+0.00005)).any()):\n",
    "                print(\"XDDD2\")\n",
    "                print(f\"XDDD2 {(torch.amax(self.t_min - tj1, dim=(1))-self.input1_val).max()}\")\n",
    "                print(self.t_min)\n",
    "                print(torch.amax(self.t_min - tj1, dim=(1)))\n",
    "                print(tj1)\n",
    "                \n",
    "        tj1_temp = (tj2-tj1)*(tj1<tj2)\n",
    "        tj2_temp = (tj1-tj2)*(tj1>=tj2)\n",
    "\n",
    "        V = tj1_temp - tj2_temp\n",
    "        \n",
    "        if((V>threshold).any()):\n",
    "            print(f\"ERROR SubSNNLayer1 V {V.max()}, thr {threshold}\")\n",
    "            print(f\"{((tj1 - self.t_min) - (tj2 - self.t_min)).max()}\")\n",
    "\n",
    "        V = (tj1 - self.t_min) - (tj2 - self.t_min)\n",
    "        if((V>threshold).any()):\n",
    "            print(f\"ERROR SubSNNLayer1 V {V.max()}, thr {threshold}\")\n",
    "        \n",
    "        ### END Check ###\n",
    "        ti = tj1 - self.t_min - (tj2 - self.t_min)  + threshold + self.t_min\n",
    "\n",
    "        ti = torch.where(ti < self.t_max, ti, self.t_max)\n",
    "\n",
    "        if self.noise > 0:\n",
    "            ti += torch.randn_like(ti) * self.noise\n",
    "        \n",
    "        return ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentitySNNLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentitySNNLayer, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "\n",
    "        max_input = max(in_ranges_max)\n",
    "        max_V = max_input\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), in_ranges_max\n",
    "\n",
    "    def forward(self, tj):\n",
    "        D_i = 0\n",
    "        threshold = self.t_max - self.t_min - D_i\n",
    "        \n",
    "        ti = tj - self.t_min  + threshold + self.t_min\n",
    "\n",
    "        ti = torch.where(ti < self.t_max, ti, self.t_max)\n",
    "\n",
    "        if self.noise > 0:\n",
    "            ti += torch.randn_like(ti) * self.noise\n",
    "        \n",
    "        return ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResidualBlockSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSNNBlock(nn.Module):\n",
    "    def __init__(self,resblock : ResidualBlock, in_channels, out_channels, stride=1, downsample=None, robustness_params = None, device = \"cuda:0\"):\n",
    "        super(ResidualSNNBlock, self).__init__()\n",
    "        conv = resblock.conv1[0]\n",
    "        bn= resblock.conv1[1]\n",
    "        bn.eval()\n",
    "        conv_fused = fuse_conv_and_bn(conv, bn)\n",
    "        self.conv1 = SpikingConv2D(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        self.device = device\n",
    "        \n",
    "        conv = resblock.conv2[0]\n",
    "        bn= resblock.conv2[1]\n",
    "        bn.eval()\n",
    "        conv_fused = fuse_conv_and_bn(conv, bn)\n",
    "        self.conv2 = SpikingConv2D(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.identity = IdentitySNNLayer()\n",
    "        self.add_layer = AddSNNLayer()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input_val, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        t_min1, t_max1, conv1_val = self.conv1.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "        self.pooling1 = MaxMinPool2D(2, t_max1.data,2).to(self.device)\n",
    "        t_min2, t_max2, conv2_val = self.conv2.set_params(t_min_prev=t_min1,t_min=t_max1, in_ranges_max=conv1_val)\n",
    "        max_out2 = t_max2 - t_min2\n",
    "        \n",
    "        if self.downsample:\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.downsample.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "            max_dummy1 = t_max1_dummy - t_min_dummy\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.downsample.set_params(t_min_prev=t_min_prev,t_min=t_min,minimal_t_max=t_max2, in_ranges_max=input_val)\n",
    "            self.pooling2 = MaxMinPool2D(2, t_max1_dummy.data,2).to(self.device)\n",
    "        else:\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.identity.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "            max_dummy1 = t_max1_dummy - t_min_dummy\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.identity.set_params(t_min_prev=t_min_prev,t_min=t_min,minimal_t_max=t_max2, in_ranges_max=input_val)\n",
    "\n",
    "        t_min2, t_max2, conv2_val = self.conv2.set_params(t_min_prev=t_min1,t_min=t_max1, minimal_t_max=t_max1_dummy, in_ranges_max=conv1_val)\n",
    "        \n",
    "        # time t_max2 and t_max1_dummy are the same\n",
    "        t_min_add = t_max2 - max(max_dummy1, max_out2)\n",
    "\n",
    "        self.t_min, self.t_max, add_val = self.add_layer.set_params(t_min_add, t_max2, conv2_val, downsample_val)\n",
    "\n",
    "        self.times = [(t_min1, t_max1, 'c'), (t_min2, t_max2, 'c'), (self.t_min, self.t_max, 'a') ]\n",
    "        return self.t_min, self.t_max, add_val\n",
    "\n",
    "    def get_main_times(self):\n",
    "        return self.times\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.pooling2(residual)\n",
    "            out = self.pooling1(out)\n",
    "        else:\n",
    "            residual = self.identity(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.add_layer(out,residual)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSNN(nn.Module):\n",
    "    def __init__(self, layer, inplanes, planes, blocks, stride=1, device = 'cuda:0'):\n",
    "        self.inplanes = inplanes\n",
    "\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            conv2d, bias_from_nn = layer[0].downsample[0], layer[0].downsample[1]\n",
    "            conv_fused = fuse_conv_and_bn(conv2d, bias_from_nn)\n",
    "            robustness_params={\n",
    "                'noise':0.0,\n",
    "                'time_bits':0,\n",
    "                'weight_bits': 0,\n",
    "                'latency_quantiles':0.0\n",
    "            }\n",
    "            downsample = SpikingConv2D(planes,\"test2\", padding='same', stride=1, device=device,robustness_params=robustness_params,kernels=conv_fused.weight.data, biases=conv_fused.bias, kernel_size=(1,1))\n",
    "            # t_min, t_max = spiking_conv2.set_params(0,1)stride\n",
    "            \n",
    "        self.layers = []\n",
    "        self.layers.append(ResidualSNNBlock(layer[0],self.inplanes,planes, 1, downsample=downsample, device=device))\n",
    "        self.inplanes = planes\n",
    "        self.blocks = blocks\n",
    "        for i in range(1, blocks):\n",
    "            self.layers.append(ResidualSNNBlock(layer[i],self.inplanes,planes, 1, downsample=None, device=device))\n",
    "        \n",
    "    \n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        tmin, tmax = t_min_prev, t_min\n",
    "        for i in range(self.blocks):\n",
    "            tmin, tmax, in_ranges_max = self.layers[i].set_params(tmin, tmax,in_ranges_max)\n",
    "        return tmin, tmax, in_ranges_max\n",
    "    \n",
    "    def get_main_times(self):\n",
    "        lst = []\n",
    "        for i in range(self.blocks):\n",
    "            lst.extend(self.layers[i].get_main_times())\n",
    "        return lst\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.blocks):\n",
    "            x = self.layers[i].forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2241,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = torch.rand(1, 5, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2242,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1 = model2.conv1(random_input)\n",
    "model_maxpool = model2.maxpool(model_conv1)\n",
    "model_resblock0 = model2.layer0[0](model_maxpool)\n",
    "model_resblock1 = model2.layer0[1](model_resblock0)\n",
    "model_resblock2 = model2.layer0[2](model_resblock1)\n",
    "model_layer0 = model2.layer0(model_maxpool)\n",
    "model_layer1 = model2.layer1(model_layer0)\n",
    "model_layer2 = model2.layer2(model_layer1)\n",
    "model_layer3 = model2.layer3(model_layer2)\n",
    "model_maxpool2 = model2.avgpool(model_layer3)\n",
    "# model2.fc.bias = nn.Parameter(torch.ones(10)*1000)\n",
    "model_linear = F.relu(model2.fc(model_maxpool2.view(model_layer3.size(0), -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingConv2D_Htanh(nn.Module):\n",
    "    def __init__(self, filters, name, X_n=1, padding='same', kernel_size=(3,3), robustness_params=None, kernels = None, device = 'cuda:0', biases = None, stride=1):\n",
    "        super(SpikingConv2D_Htanh, self).__init__()\n",
    "        if robustness_params is None:\n",
    "            robustness_params = {}\n",
    "        kernels_pos = torch.concat((kernels,-kernels), dim=1 )\n",
    "        if biases is not None:\n",
    "            biases_pos = biases\n",
    "        else:\n",
    "            biases_pos = None\n",
    "        \n",
    "        kernels_neg = torch.concat((-kernels,kernels), dim=1 )\n",
    "        if biases is not None:\n",
    "            biases_neg = -biases\n",
    "        else:\n",
    "            biases_neg = None\n",
    "\n",
    "        kernels_new = torch.concat((kernels_pos, kernels_neg), dim=0)\n",
    "        biases_new = torch.concat((biases_pos, biases_neg))\n",
    "        print(biases_new.shape)\n",
    "        self.conv_first = SpikingConv2D(2*filters, name, device = device, padding=padding, stride=stride, kernel_size=kernel_size,robustness_params=robustness_params, kernels=kernels_new, biases=biases_new)\n",
    "        \n",
    "        kernels_new2 = torch.concat((kernels_pos, kernels_neg), dim=0)\n",
    "        biases_new2 = torch.concat((biases_pos, biases_neg)) - 1\n",
    "        self.conv_second = SpikingConv2D(2*filters, name, device = device, padding=padding, stride=stride, kernel_size=kernel_size,robustness_params=robustness_params, kernels=kernels_new2, biases=biases_new2)\n",
    "        self.sub = SubSNNLayer()\n",
    "        self.filters = filters*2\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        if(in_ranges_max.shape[0] != self.filters):\n",
    "            in_ranges_max = torch.concat((in_ranges_max,torch.zeros(in_ranges_max.shape)))\n",
    "        tmin1, tmax1, first_val = self.conv_first.set_params(t_min_prev, t_min, in_ranges_max, minimal_t_max-1)\n",
    "        tmin2, tmax2, second_val = self.conv_second.set_params(t_min_prev, t_min, in_ranges_max, tmax1)\n",
    "\n",
    "        tmin1, tmax1, first_val = self.conv_first.set_params(t_min_prev, t_min, in_ranges_max, tmax2)\n",
    "        self.t_max = tmax1\n",
    "        tmins, tmaxs, sub_val = self.sub.set_params(t_min, tmax1, first_val,second_val)\n",
    "        tmaxs = tmins+1\n",
    "        self.sub.t_max = tmaxs\n",
    "        self.t_max = tmaxs\n",
    "        # Returning for function signature consistency\n",
    "        return tmins, self.t_max, torch.minimum(sub_val, torch.ones(sub_val.shape))\n",
    "        # return tmin1, self.t_max, torch.minimum(first_val, torch.ones(first_val.shape))\n",
    "\n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times tj, output spiking times ti. \n",
    "        \"\"\"\n",
    "        tj1 = self.conv_first(tj)\n",
    "        tj2 = self.conv_second(tj)\n",
    "        tj_sub = self.sub(tj1, tj2)\n",
    "\n",
    "        return tj_sub\n",
    "        # return tj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddSNNLayer_all(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, bias=0):\n",
    "        super(AddSNNLayer_all, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        self.B = bias # bias for all inputs (for Hard tanh)\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input1_val, input2_val, minimal_t_max = 0):\n",
    "        if input2_val.shape[0] != input1_val.shape[0]:\n",
    "            input2_val = torch.concat((input2_val, torch.zeros(input2_val.shape)))\n",
    "        output_val = input1_val + input2_val + self.B\n",
    "        max_V = max(output_val)\n",
    "        self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
    "        self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
    "        return t_min, max(t_min + self.B_n*max_V, minimal_t_max), output_val\n",
    "\n",
    "    def forward(self, tj1, tj2):\n",
    "\n",
    "        self.channels = tj1.shape[1]//2\n",
    "\n",
    "        D_i = 0\n",
    "        threshold = self.t_max - self.t_min - D_i\n",
    "        \n",
    "        ti = torch.concat((tj1[0, :self.channels] + tj2[0, :self.channels] - tj1[0, self.channels:] - tj2[0, self.channels:], \n",
    "                           tj1[0, self.channels:] + tj2[0, self.channels:] - tj1[0, :self.channels] - tj2[0, :self.channels])) + self.B*(1) + threshold + self.t_min\n",
    "\n",
    "        ti = torch.where(ti < self.t_max, ti, self.t_max)\n",
    "\n",
    "        if self.noise > 0:\n",
    "            ti += torch.randn_like(ti) * self.noise\n",
    "        \n",
    "        return ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddSNNLayer_Htanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AddSNNLayer_Htanh, self).__init__()\n",
    "        self.noise = 0\n",
    "        self.B_n = 1\n",
    "        self.first = AddSNNLayer_all()\n",
    "        self.second = AddSNNLayer_all(1)\n",
    "        self.sub = SubSNNLayer()\n",
    "        pass\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input1_val, input2_val, minimal_t_max = 0):\n",
    "        tmin1, tmax1, first_val = self.first.set_params(t_min_prev, t_min,input1_val, input2_val, minimal_t_max=t_min+1)\n",
    "        tmin2, tmax2, second_val = self.second.set_params(t_min_prev, t_min,input1_val, input2_val, minimal_t_max=tmax1)\n",
    "\n",
    "        tmin1, tmax1, first_val = self.first.set_params(t_min_prev, t_min,input1_val, input2_val, minimal_t_max=tmax2)\n",
    "\n",
    "        tmins, tmaxs, sub_val = self.sub.set_params(t_min, tmax1, first_val,second_val) ## t_min as angument do nothing\n",
    "        self.sub.t_max = tmaxs\n",
    "        return tmins, tmaxs, torch.minimum(sub_val,torch.ones(sub_val.shape))\n",
    "\n",
    "    def forward(self, tj1, tj2):\n",
    "        tj_first = self.first(tj1, tj2)\n",
    "        tj_second = self.second(tj1, tj2)\n",
    "        tj_sub = self.sub(tj_first, tj_second)\n",
    "        return tj_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingConv2D_all(nn.Module):\n",
    "    def __init__(self, filters, name, X_n=1, padding='same', kernel_size=(3,3), robustness_params=None, kernels = None, device = 'cuda:0', biases = None, stride=1):\n",
    "        super(SpikingConv2D_all, self).__init__()\n",
    "        if robustness_params is None:\n",
    "            robustness_params = {}\n",
    "        kernels_pos = torch.concat((kernels,-kernels), dim=1 )\n",
    "        if biases is not None:\n",
    "            biases_pos = biases\n",
    "        else:\n",
    "            biases_pos = None\n",
    "        kernels_new = kernels_pos\n",
    "        biases_new = biases_pos\n",
    "        self.conv_first = SpikingConv2D(filters, name, device = device, padding=padding, stride=stride, kernel_size=kernel_size,robustness_params=robustness_params, kernels=kernels_new, biases=biases_new)\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        tmin1, tmax1, first_val = self.conv_first.set_params(t_min_prev, t_min, in_ranges_max)\n",
    "        \n",
    "        # Returning for function signature consistency\n",
    "        return tmin1, tmax1, first_val\n",
    "        # return tmin1, self.t_max, torch.minimum(first_val, torch.ones(first_val.shape))\n",
    "\n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times tj, output spiking times ti. \n",
    "        \"\"\"\n",
    "        tj1 = self.conv_first(tj)\n",
    "        return tj1\n",
    "        # return tj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSNNBlock_all(nn.Module):\n",
    "    def __init__(self,resblock : ResidualBlock, in_channels, out_channels, stride=1, downsample=None, robustness_params = None, device = \"cuda:0\", end_maxpool = False):\n",
    "        super(ResidualSNNBlock_all, self).__init__()\n",
    "        conv = resblock.conv1[0]\n",
    "        bn= resblock.conv1[1]\n",
    "        bn.eval()\n",
    "        conv_fused = fuse_conv_and_bn(conv, bn)\n",
    "        if (downsample is not None):\n",
    "            self.conv1 = SpikingConv2D_all(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        else:\n",
    "            self.conv1 = SpikingConv2D_Htanh(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        self.device = device\n",
    "        \n",
    "        conv = resblock.conv2[0]\n",
    "        bn= resblock.conv2[1]\n",
    "        bn.eval()\n",
    "        conv_fused = fuse_conv_and_bn(conv, bn)\n",
    "        self.conv2 = SpikingConv2D_Htanh(out_channels, \"temp1\", device=device, padding=(1,1), stride=stride, kernel_size=(3,3),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.identity = IdentitySNNLayer()\n",
    "        if end_maxpool:\n",
    "            self.add_layer = AddSNNLayer_all()\n",
    "        else:\n",
    "            self.add_layer = AddSNNLayer_Htanh()\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, input_val, minimal_t_max = 0):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max, J_ij (kernel) and vartheta_i (threshold) parameters of this layer.\n",
    "        \"\"\"\n",
    "        t_min1, t_max1, conv1_val = self.conv1.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "        self.t_max1 = t_max1\n",
    "        self.pooling1 = MaxMinPool2D(2, t_max1.data,2).to(self.device)\n",
    "        t_min2, t_max2, conv2_val = self.conv2.set_params(t_min_prev=t_min1,t_min=t_max1, in_ranges_max=conv1_val)\n",
    "        max_out2 = t_max2 - t_min2\n",
    "        \n",
    "        if self.downsample:\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.downsample.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "            max_dummy1 = t_max1_dummy - t_min_dummy\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.downsample.set_params(t_min_prev=t_min_prev,t_min=t_min,minimal_t_max=t_max2, in_ranges_max=input_val)\n",
    "            self.t_max1_dummy = t_max1_dummy\n",
    "            self.pooling2 = MaxMinPool2D(2, t_max1_dummy.data,2).to(self.device)\n",
    "        else:\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.identity.set_params(t_min_prev=t_min_prev,t_min=t_min, in_ranges_max=input_val)\n",
    "            max_dummy1 = t_max1_dummy - t_min_dummy\n",
    "            t_min_dummy, t_max1_dummy, downsample_val = self.identity.set_params(t_min_prev=t_min_prev,t_min=t_min,minimal_t_max=t_max2, in_ranges_max=input_val)\n",
    "\n",
    "        t_min2, t_max2, conv2_val = self.conv2.set_params(t_min_prev=t_min1,t_min=t_max1, minimal_t_max=t_max1_dummy, in_ranges_max=conv1_val)\n",
    "        \n",
    "        # time t_max2 and t_max1_dummy are the same\n",
    "        t_min_add = t_max2 - max(max_dummy1, max_out2)\n",
    "\n",
    "        self.t_min, self.t_max, add_val = self.add_layer.set_params(t_min_add, t_max2, conv2_val, downsample_val)\n",
    "\n",
    "        self.times = [(t_min1, t_max1, 'c'), (t_min2, t_max2, 'c'), (self.t_min, self.t_max, 'a') ]\n",
    "        return self.t_min, self.t_max, add_val\n",
    "\n",
    "    def get_main_times(self):\n",
    "        return self.times\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.pooling2(residual)\n",
    "            out = self.pooling1(out)\n",
    "            residual = torch.concat((residual,torch.ones(residual.shape)*self.t_max1_dummy), dim=1)\n",
    "            out = torch.concat((out,torch.ones(out.shape)*self.t_max1), dim=1)\n",
    "        else:\n",
    "            residual = self.identity(x)\n",
    "        print(out.shape)\n",
    "        out = self.conv2(out)\n",
    "        out = self.add_layer(out,residual) # no need for adding negative part\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerSNN_all(nn.Module):\n",
    "    def __init__(self, layer, inplanes, planes, blocks, stride=1, device = 'cuda:0', end_maxpool = False):\n",
    "        self.inplanes = inplanes\n",
    "\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            conv2d, bias_from_nn = layer[0].downsample[0], layer[0].downsample[1]\n",
    "            conv_fused = fuse_conv_and_bn(conv2d, bias_from_nn)\n",
    "            robustness_params={\n",
    "                'noise':0.0,\n",
    "                'time_bits':0,\n",
    "                'weight_bits': 0,\n",
    "                'latency_quantiles':0.0\n",
    "            }\n",
    "            downsample = SpikingConv2D_all(planes,\"test2\", padding='same', stride=1, device=device,robustness_params=robustness_params,kernels=conv_fused.weight.data, biases=conv_fused.bias, kernel_size=(1,1))\n",
    "            # t_min, t_max = spiking_conv2.set_params(0,1)stride\n",
    "            \n",
    "        self.layers = []\n",
    "        self.layers.append(ResidualSNNBlock_all(layer[0],self.inplanes,planes, 1, downsample=downsample, device=device))\n",
    "        self.inplanes = planes\n",
    "        self.blocks = blocks\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                self.layers.append(ResidualSNNBlock_all(layer[i],self.inplanes,planes, 1, downsample=None, device=device, end_maxpool = True))\n",
    "            else:\n",
    "                self.layers.append(ResidualSNNBlock_all(layer[i],self.inplanes,planes, 1, downsample=None, device=device))\n",
    "            \n",
    "        \n",
    "    \n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max, minimal_t_max = 0):\n",
    "        tmin, tmax = t_min_prev, t_min\n",
    "        for i in range(self.blocks):\n",
    "            tmin, tmax, in_ranges_max = self.layers[i].set_params(tmin, tmax,in_ranges_max)\n",
    "        return tmin, tmax, in_ranges_max\n",
    "    \n",
    "    def get_main_times(self):\n",
    "        lst = []\n",
    "        for i in range(self.blocks):\n",
    "            lst.extend(self.layers[i].get_main_times())\n",
    "        return lst\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.blocks):\n",
    "            print(i)\n",
    "            x = self.layers[i].forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2249,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_params={\n",
    "    'noise':0.0,\n",
    "    'time_bits':0,\n",
    "    'weight_bits': 0,\n",
    "    'latency_quantiles':0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(4.5586, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "conv_fused = fuse_conv_and_bn(model2.conv1[0], model2.conv1[1])\n",
    "conv_first = SpikingConv2D(64, \"temp1\", device = 'cpu', padding=(3,3), stride=2, kernel_size=(7,7),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "max_vect = torch.tensor([1,1,1,1,1])\n",
    "tmin, tmax, max_vect = conv_first.set_params(0,1,max_vect)\n",
    "print(tmin, tmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "model_conv1 = model2.conv1(random_input)\n",
    "print(model_conv1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.2543e-04, 0.0000e+00, 6.4826e-02, 2.8124e-02, 0.0000e+00, 1.0669e+00,\n",
      "        3.5316e-07, 0.0000e+00, 1.8713e-03, 0.0000e+00, 1.5711e-05, 5.7183e-05,\n",
      "        1.7728e-06, 1.9651e-05, 0.0000e+00, 1.4251e-02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.4909e-02, 0.0000e+00, 3.2300e-02, 1.6224e+00,\n",
      "        0.0000e+00, 3.6996e-02, 5.6862e-02, 1.2020e-06, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 5.0145e-06, 0.0000e+00, 8.9199e-03, 0.0000e+00,\n",
      "        1.5293e-02, 0.0000e+00, 4.6955e-02, 0.0000e+00, 5.3887e-03, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2771e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.5586e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5823e-05, 3.8386e-03, 5.4359e-02,\n",
      "        0.0000e+00, 0.0000e+00, 5.7028e-03, 1.4734e+00],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(max_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tworzenie przykładowego array'a\n",
    "values = max_vect.detach().numpy()\n",
    "labels = [ i for i in range(len(max_vect))]\n",
    "\n",
    "# Tworzenie wykresu słupkowego\n",
    "plt.bar(labels, values, color='skyblue')\n",
    "\n",
    "# Dodanie etykiet\n",
    "plt.ylabel('$x_{max}$')\n",
    "plt.xlabel('Channel number')\n",
    "plt.title('maksymalne wartości każdego kanału')\n",
    "\n",
    "# Wyświetlenie wykresu\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 224, 224])\n",
      "tensor(1.1027e-06, grad_fn=<MaxBackward1>)\n",
      "torch.Size([1, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "SNN_input = 1 - random_input\n",
    "\n",
    "# SNN_input = torch.concat((SNN_input, torch.ones(SNN_input.shape)),dim=1)\n",
    "print(SNN_input.shape)\n",
    "model2.conv1.eval() ## Important eval for batch normalization\n",
    "out1 = conv_first(SNN_input.to(\"cpu\"))\n",
    "# print(out1)\n",
    "out1_x = model2.conv1(random_input)\n",
    "temp = (conv_first.t_max - out1)\n",
    "# print((temp[0,:64] - temp[0,64:] - model_conv1).abs().max())\n",
    "print((temp - model_conv1).abs().max())\n",
    "print(out1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.2543e-04, 0.0000e+00, 6.4826e-02, 2.8124e-02, 0.0000e+00, 1.0669e+00,\n",
      "        3.5316e-07, 0.0000e+00, 1.8713e-03, 0.0000e+00, 1.5711e-05, 5.7183e-05,\n",
      "        1.7728e-06, 1.9651e-05, 0.0000e+00, 1.4251e-02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.4909e-02, 0.0000e+00, 3.2300e-02, 1.6224e+00,\n",
      "        0.0000e+00, 3.6996e-02, 5.6862e-02, 1.2020e-06, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 5.0145e-06, 0.0000e+00, 8.9199e-03, 0.0000e+00,\n",
      "        1.5293e-02, 0.0000e+00, 4.6955e-02, 0.0000e+00, 5.3887e-03, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2771e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.5586e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5823e-05, 3.8386e-03, 5.4359e-02,\n",
      "        0.0000e+00, 0.0000e+00, 5.7028e-03, 1.4734e+00],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(max_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.amax(tmax -out1, dim=(2, 3))<=max_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1219e-04, 0.0000e+00, 3.0036e-02, 1.8811e-02, 0.0000e+00, 4.4011e-01,\n",
      "         0.0000e+00, 0.0000e+00, 1.7166e-05, 0.0000e+00, 1.3351e-05, 2.4796e-05,\n",
      "         1.9073e-06, 1.7166e-05, 0.0000e+00, 7.5407e-03, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.3638e-04, 0.0000e+00, 1.4690e-02, 4.0922e-01,\n",
      "         0.0000e+00, 1.2297e-02, 1.2832e-02, 9.5367e-07, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.8610e-06, 0.0000e+00, 6.9256e-03, 0.0000e+00,\n",
      "         9.3336e-03, 0.0000e+00, 1.0137e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1692e-01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0678e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 7.6294e-06, 2.8620e-03, 3.1489e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0477e-01]],\n",
      "       grad_fn=<AmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.amax(tmax -out1, dim=(2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1027e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "pool = MaxMinPool2D(3, tmax.data,2,1).to(\"cpu\")\n",
    "\n",
    "out2 = pool(out1)\n",
    "\n",
    "print(((tmax - out2) - model_maxpool).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2259,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsnn2 = AddSNNLayer_all(1)\n",
    "addsnn1 = AddSNNLayer_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 224, 224])\n",
      "tensor(2.3842e-07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "SNN_input1 = 1 - random_input +1\n",
    "SNN_input2 = 1 - random_input +1\n",
    "\n",
    "val_in1, val_in2 = torch.concat((torch.ones(5), torch.zeros(5))),torch.concat((torch.ones(5), torch.zeros(5)))\n",
    "tmin1, tmax1, val1 = addsnn1.set_params(0+1,1+1,val_in1,val_in2)\n",
    "tmin2, tmax2, val2 = addsnn2.set_params(0+1,1+1,val_in1,val_in2,tmax1)\n",
    "tmin1, tmax1, val1 = addsnn1.set_params(0+1,1+1,val_in1,val_in2,tmax2)\n",
    "\n",
    "outadd1 = addsnn1(torch.concat((SNN_input1, torch.ones(SNN_input1.shape)+1),dim=1),torch.concat((SNN_input2, torch.ones(SNN_input2.shape)+1),dim=1))\n",
    "print(outadd1.shape)\n",
    "print(((tmax1 - outadd1)[:5] - (tmax1 - outadd1)[5:] - F.relu(random_input*2)).abs().max())# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 224, 224])\n",
      "tensor(2.3842e-07)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "outadd2 = addsnn2(torch.concat((SNN_input1, torch.ones(SNN_input1.shape)+1),dim=1),torch.concat((SNN_input2, torch.ones(SNN_input2.shape)+1),dim=1))\n",
    "print(outadd2.shape)\n",
    "print(((tmax1 - outadd2)[:5] - (tmax1 - outadd2)[5:] - F.relu(random_input*2-1)).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9999, 2.0000, 2.0000, 2.0000, 1.9999, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.amax(tmax1 - outadd1, dim=(1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2264,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = SubSNNLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "tmins, tmaxs, sub_val = sub.set_params(0, tmax1, val1,val2)\n",
    "# sub.t_max = tmins+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 224, 224])\n",
      "tensor(2.3842e-07)\n"
     ]
    }
   ],
   "source": [
    "outsub = sub(outadd1,outadd2)\n",
    "print(((sub.t_max-outsub)[:5] - (sub.t_max-outsub)[5:] - F.hardtanh(random_input*2)).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resblock test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "resblocksnn = ResidualSNNBlock_all(model2.layer0[0],64,64, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.8919, grad_fn=<AddBackward0>) tensor(20.5207, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmin2, tmax2, max_vect2 = resblocksnn.set_params(tmin, tmax, torch.concat((max_vect, torch.zeros(max_vect.shape))))\n",
    "print(tmin2, tmax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "print(torch.concat((out2, torch.ones(out2.shape) * tmin),dim=1).shape)\n",
    "out3res = resblocksnn(torch.concat((out2, torch.ones(out2.shape) * tmax),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(3.1665e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((tmax2 - out3res)[:64].max())\n",
    "print(((tmax2 - out3res)[:64] - (tmax2 - out3res)[64:]  - model_resblock0).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5586, grad_fn=<AddBackward0>) tensor(20.5207, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(tmax, tmax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "tensor(24.6481, grad_fn=<AddBackward0>) tensor(25.7118, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "resblocksnn2 = ResidualSNNBlock_all(model2.layer0[1],64,64, device='cpu')\n",
    "tmin2, tmax2, max_vect2 = resblocksnn2.set_params(tmin2, tmax2, max_vect2)\n",
    "print(tmin2, tmax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "out4res = resblocksnn2(out3res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(5.6699e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((tmax2 - out4res)[:64].max())\n",
    "print(((tmax2 - out4res)[:64] - (tmax2 - out4res)[64:]  - model_resblock1).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "tensor(30.0252, grad_fn=<AddBackward0>) tensor(31.1081, grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([128, 56, 56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "resblocksnn3 = ResidualSNNBlock_all(model2.layer0[2],64,64, device='cpu')\n",
    "tmin2, tmax2, max_vect2 = resblocksnn3.set_params(tmin2, tmax2, max_vect2)\n",
    "print(tmin2, tmax2)\n",
    "out5res = resblocksnn3(out4res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(7.9758e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((tmax2 - out5res)[:64].max())\n",
    "print(((tmax2 - out5res)[:64] - (tmax2 - out5res)[64:]  - model_resblock2).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy = None\n",
    "\n",
    "# resblockSNN = ResidualSNNBlock(model2.layer0[0],64,64, downsample=dummy, device='cpu')\n",
    "# tmin, tmax, max_vect = resblockSNN.set_params(0,1, max_vect)\n",
    "# print(tmin,tmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "tensor(30.0252, grad_fn=<AddBackward0>) tensor(31.1081, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "layer0SNN = LayerSNN_all(model2.layer0, 64, 64, 3,device = 'cpu')\n",
    "tmax_prev = tmax\n",
    "tmin, tmax, max_vect = layer0SNN.set_params(tmin, tmax, torch.concat((max_vect, torch.zeros(max_vect.shape))))\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([128, 56, 56])\n",
      "1\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([128, 56, 56])\n",
      "2\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([1, 128, 56, 56])\n",
      "torch.Size([128, 56, 56])\n",
      "torch.Size([128, 56, 56])\n",
      "tensor(7.9758e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_3 = layer0SNN.forward(torch.concat((out2, torch.ones(out2.shape) * tmax_prev),dim=1))\n",
    "print(out_3.shape)\n",
    "print(((tmax - out_3)[:64] - (tmax - out_3)[64:] - model_layer0).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "tensor(64.6347, grad_fn=<AddBackward0>) tensor(65.6811, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "layer1SNN = LayerSNN_all(model2.layer1, 64, 128, 4,device = 'cpu')\n",
    "tmin, tmax, max_vect = layer1SNN.set_params(tmin, tmax, max_vect)\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([256, 28, 28])\n",
      "1\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([256, 28, 28])\n",
      "2\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([256, 28, 28])\n",
      "3\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([256, 28, 28])\n",
      "torch.Size([256, 28, 28])\n",
      "tensor(2.1899e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_4 = layer1SNN.forward(out_3)\n",
    "print((tmax - out_4).shape)\n",
    "print(((tmax - out_4)[ :128] - (tmax - out_4)[ 128:] - model_layer1).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(121.2562, grad_fn=<AddBackward0>) tensor(122.4984, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer2SNN = LayerSNN_all(model2.layer2, 128, 256, 6,device = 'cpu')\n",
    "tmin, tmax, max_vect = layer2SNN.set_params(tmin, tmax, max_vect)\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([512, 14, 14])\n",
      "1\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([512, 14, 14])\n",
      "2\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([512, 14, 14])\n",
      "3\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([512, 14, 14])\n",
      "4\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([512, 14, 14])\n",
      "5\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([512, 14, 14])\n",
      "tensor(5.6906e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_5 = layer2SNN.forward(out_4)\n",
    "\n",
    "print(((tmax - out_5)[:256] - (tmax - out_5)[256:] - model_layer2).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\1310482246.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\3502596109.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(205.6349, grad_fn=<AddBackward0>) tensor(207.6349, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer3SNN = LayerSNN_all(model2.layer3, 256, 512, 3,device = 'cpu',end_maxpool=True)\n",
    "tmin, tmax, max_vect = layer3SNN.set_params(tmin, tmax, max_vect)\n",
    "print(tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "torch.Size([1024, 7, 7])\n",
      "1\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "torch.Size([1024, 7, 7])\n",
      "2\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "tensor(6.8963e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_6 = layer3SNN.forward(out_5)\n",
    "\n",
    "print(((tmax - out_6)[:512] - model_layer3).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8963e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "pool2 = MaxMinPool2D(7, tmax.data,1,0).to(\"cpu\")\n",
    "\n",
    "out7 = pool2(out_6[:512])\n",
    "\n",
    "print(((tmax - out7) - model_maxpool2).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(207.6349, grad_fn=<AddBackward0>) tensor(222.0959, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "spiking_dense = SpikingDense(10,\"test\",robustness_params=robustness_params)\n",
    "weights = model2.fc.weight.T\n",
    "biases = model2.fc.bias\n",
    "spiking_dense.build((512,),weights, biases)\n",
    "tmin_, tmax_, max_vect_ = spiking_dense.set_params(tmin, tmax, max_vect[:512])\n",
    "print(tmin_, tmax_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "tensor(8.7213e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out8 = spiking_dense(out7.view(out7.size(0), -1))\n",
    "\n",
    "print((tmax_ - out8 - model_linear).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2289,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_lst = [layer0SNN, layer1SNN, layer2SNN, layer3SNN]\n",
    "ll = []\n",
    "for i in layer_lst:\n",
    "    ll.extend(i.get_main_times())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(8.9117, grad_fn=<AddBackward0>), tensor(9.9117, grad_fn=<AddBackward0>), 'c'), (tensor(11.2631, grad_fn=<AddBackward0>), tensor(12.2631, grad_fn=<AddBackward0>), 'c'), (tensor(16.8919, grad_fn=<AddBackward0>), tensor(20.5207, grad_fn=<AddBackward0>), 'a'), (tensor(20.5208, grad_fn=<AddBackward0>), tensor(21.5208, grad_fn=<AddBackward0>), 'c'), (tensor(21.5845, grad_fn=<AddBackward0>), tensor(22.5845, grad_fn=<AddBackward0>), 'c'), (tensor(24.6481, grad_fn=<AddBackward0>), tensor(25.7118, grad_fn=<AddBackward0>), 'a'), (tensor(25.8563, grad_fn=<AddBackward0>), tensor(26.8563, grad_fn=<AddBackward0>), 'c'), (tensor(26.9423, grad_fn=<AddBackward0>), tensor(27.9423, grad_fn=<AddBackward0>), 'c'), (tensor(30.0252, grad_fn=<AddBackward0>), tensor(31.1081, grad_fn=<AddBackward0>), 'a'), (tensor(31.1081, grad_fn=<AddBackward0>), tensor(33.9411, grad_fn=<AddBackward0>), 'c'), (tensor(41.7274, grad_fn=<AddBackward0>), tensor(42.7274, grad_fn=<AddBackward0>), 'c'), (tensor(46.9956, grad_fn=<AddBackward0>), tensor(50.2639, grad_fn=<AddBackward0>), 'a'), (tensor(50.2640, grad_fn=<AddBackward0>), tensor(51.2640, grad_fn=<AddBackward0>), 'c'), (tensor(51.3103, grad_fn=<AddBackward0>), tensor(52.3103, grad_fn=<AddBackward0>), 'c'), (tensor(54.3566, grad_fn=<AddBackward0>), tensor(55.4029, grad_fn=<AddBackward0>), 'a'), (tensor(55.4030, grad_fn=<AddBackward0>), tensor(56.4030, grad_fn=<AddBackward0>), 'c'), (tensor(56.4493, grad_fn=<AddBackward0>), tensor(57.4493, grad_fn=<AddBackward0>), 'c'), (tensor(59.4956, grad_fn=<AddBackward0>), tensor(60.5419, grad_fn=<AddBackward0>), 'a'), (tensor(60.5420, grad_fn=<AddBackward0>), tensor(61.5420, grad_fn=<AddBackward0>), 'c'), (tensor(61.5884, grad_fn=<AddBackward0>), tensor(62.5884, grad_fn=<AddBackward0>), 'c'), (tensor(64.6347, grad_fn=<AddBackward0>), tensor(65.6811, grad_fn=<AddBackward0>), 'a'), (tensor(65.6811, grad_fn=<AddBackward0>), tensor(71.2348, grad_fn=<AddBackward0>), 'c'), (tensor(82.5302, grad_fn=<AddBackward0>), tensor(83.5302, grad_fn=<AddBackward0>), 'c'), (tensor(88.6710, grad_fn=<AddBackward0>), tensor(92.8119, grad_fn=<AddBackward0>), 'a'), (tensor(93.0471, grad_fn=<AddBackward0>), tensor(94.0471, grad_fn=<AddBackward0>), 'c'), (tensor(94.2617, grad_fn=<AddBackward0>), tensor(95.2617, grad_fn=<AddBackward0>), 'c'), (tensor(97.4762, grad_fn=<AddBackward0>), tensor(98.6908, grad_fn=<AddBackward0>), 'a'), (tensor(98.9446, grad_fn=<AddBackward0>), tensor(99.9446, grad_fn=<AddBackward0>), 'c'), (tensor(100.1930, grad_fn=<AddBackward0>), tensor(101.1930, grad_fn=<AddBackward0>), 'c'), (tensor(103.4414, grad_fn=<AddBackward0>), tensor(104.6898, grad_fn=<AddBackward0>), 'a'), (tensor(104.9207, grad_fn=<AddBackward0>), tensor(105.9207, grad_fn=<AddBackward0>), 'c'), (tensor(106.1677, grad_fn=<AddBackward0>), tensor(107.1677, grad_fn=<AddBackward0>), 'c'), (tensor(109.2934, grad_fn=<AddBackward0>), tensor(110.4192, grad_fn=<AddBackward0>), 'a'), (tensor(110.6242, grad_fn=<AddBackward0>), tensor(111.6242, grad_fn=<AddBackward0>), 'c'), (tensor(111.9529, grad_fn=<AddBackward0>), tensor(112.9529, grad_fn=<AddBackward0>), 'c'), (tensor(115.1715, grad_fn=<AddBackward0>), tensor(116.3901, grad_fn=<AddBackward0>), 'a'), (tensor(116.6608, grad_fn=<AddBackward0>), tensor(117.6608, grad_fn=<AddBackward0>), 'c'), (tensor(118.0139, grad_fn=<AddBackward0>), tensor(119.0139, grad_fn=<AddBackward0>), 'c'), (tensor(121.2562, grad_fn=<AddBackward0>), tensor(122.4984, grad_fn=<AddBackward0>), 'a'), (tensor(122.4984, grad_fn=<AddBackward0>), tensor(129.7513, grad_fn=<AddBackward0>), 'c'), (tensor(159.6793, grad_fn=<AddBackward0>), tensor(160.6793, grad_fn=<AddBackward0>), 'c'), (tensor(165.0820, grad_fn=<AddBackward0>), tensor(168.4847, grad_fn=<AddBackward0>), 'a'), (tensor(169.5437, grad_fn=<AddBackward0>), tensor(170.5437, grad_fn=<AddBackward0>), 'c'), (tensor(173.0313, grad_fn=<AddBackward0>), tensor(174.0313, grad_fn=<AddBackward0>), 'c'), (tensor(177.0313, grad_fn=<AddBackward0>), tensor(179.0313, grad_fn=<AddBackward0>), 'a'), (tensor(195.4436, grad_fn=<AddBackward0>), tensor(196.4436, grad_fn=<AddBackward0>), 'c'), (tensor(204.6349, grad_fn=<AddBackward0>), tensor(205.6349, grad_fn=<AddBackward0>), 'c'), (tensor(205.6349, grad_fn=<AddBackward0>), tensor(207.6349, grad_fn=<AddBackward0>), 'a')]\n"
     ]
    }
   ],
   "source": [
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2291,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = [i[1].detach().numpy() for i in ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'czas')"
      ]
     },
     "execution_count": 2292,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(tt)\n",
    "plt.xlabel(\"Nr. warstwy\")\n",
    "plt.ylabel(\"czas\")\n",
    "# plt.ylim([0,2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2293,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt2 = [(tt[i])/(tt[i-1]) for i in range(2,len(tt))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b4d1728c70>]"
      ]
     },
     "execution_count": 2294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingDense_positive_tanH(nn.Module):\n",
    "    \"\"\"\n",
    "    from positive to all and Hard tanh\n",
    "    \"\"\"\n",
    "    def __init__(self, units, input_units, name, weights, biases, X_n=1, outputLayer=False, robustness_params={}, input_dim=None,\n",
    "                 kernel_regularizer=None, kernel_initializer=None):\n",
    "        super().__init__()\n",
    "        W = torch.concatenate((weights.T, -weights.T),dim=1)\n",
    "        b1 = torch.concatenate((biases, -biases))\n",
    "        b2 = torch.concatenate((biases, -biases)) - 1\n",
    "        self.first = SpikingDense(2*units,\"test\",robustness_params=robustness_params)\n",
    "        self.second = SpikingDense(2*units,\"test\",robustness_params=robustness_params)\n",
    "        self.first.build((input_units,), W, b1)\n",
    "        self.second.build((input_units,), W, b2)\n",
    "        self.sub_layer = SubSNNLayer()\n",
    "\n",
    "\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max parameters of this layer. Alpha is fixed at 1.\n",
    "        \"\"\"\n",
    "        tmin1, tmax1, first_val = self.first.set_params(t_min_prev, t_min,in_ranges_max, minimal_t_max=t_min+1)\n",
    "        tmin2, tmax2, second_val = self.second.set_params(t_min_prev, t_min,in_ranges_max, minimal_t_max=tmax1)\n",
    "\n",
    "        tmin1, tmax1, first_val = self.first.set_params(t_min_prev, t_min,in_ranges_max, minimal_t_max=tmax2)\n",
    "\n",
    "        tmins, tmaxs, sub_val = self.sub_layer.set_params(t_min, tmax1, first_val,second_val) ## t_min as angument do nothing\n",
    "        self.sub_layer.t_max = tmins+1\n",
    "        return tmins, tmins+1, sub_val\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times `tj`, output spiking times `ti` or membrane potential value for the output layer.\n",
    "        \"\"\"\n",
    "        # Call the custom spiking logic\n",
    "        out1 = self.first(tj)\n",
    "        out2 = self.second(tj)\n",
    "        sub_ = self.sub_layer(out1,out2)\n",
    "        return sub_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "input_ttfs = out7.view(out7.size(0), -1)\n",
    "input_x = tmax - input_ttfs\n",
    "t_min__ = tmin\n",
    "t_max__ = tmax\n",
    "max_vect__ = max_vect\n",
    "print(input_ttfs.shape)\n",
    "print(max_vect__.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2297,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SpikingDense_positive_tanH(10,512, '',model2.fc.weight, model2.fc.bias,robustness_params=robustness_params)\n",
    "layer_fc = model2.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\4177980474.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "t_min, t_max, max_vect_temp = layer.set_params(t_min__, t_max__, max_vect__[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 20])\n",
      "tensor(8.0019e-06, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_tanh = layer.forward(input_ttfs)\n",
    "out_x = model2.fc.forward(input_x)\n",
    "\n",
    "print(((t_max - out_tanh)[0,:10] - (t_max - out_tanh)[0,10:] - F.hardtanh(out_x)).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingDense_all_all(nn.Module):\n",
    "    \"\"\"\n",
    "    from all to all (pure linear layer)\n",
    "    \"\"\"\n",
    "    def __init__(self, units, input_units, name, weights, biases, X_n=1, outputLayer=False, robustness_params={}, input_dim=None,\n",
    "                 kernel_regularizer=None, kernel_initializer=None):\n",
    "        super().__init__()\n",
    "        W1 = torch.concatenate((weights.T, -weights.T),dim=1)\n",
    "        W2 = torch.concatenate((-weights.T, weights.T),dim=1)\n",
    "        W = torch.concatenate((W1,W2),dim=0)\n",
    "        b1 = torch.concatenate((biases, -biases))\n",
    "        self.first = SpikingDense(2*units,\"test\",robustness_params=robustness_params)\n",
    "        self.first.build((2*input_units,), W, b1)\n",
    "\n",
    "\n",
    "\n",
    "    def set_params(self, t_min_prev, t_min, in_ranges_max):\n",
    "        \"\"\"\n",
    "        Set t_min_prev, t_min, t_max parameters of this layer. Alpha is fixed at 1.\n",
    "        \"\"\"\n",
    "        tmin1, tmax1, first_val = self.first.set_params(t_min_prev, t_min,in_ranges_max, minimal_t_max=t_min+1)\n",
    "        return tmin1, tmax1, first_val\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        \"\"\"\n",
    "        Input spiking times `tj`, output spiking times `ti` or membrane potential value for the output layer.\n",
    "        \"\"\"\n",
    "        # Call the custom spiking logic\n",
    "        out1 = self.first(tj)\n",
    "        return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sh = list(input_ttfs.shape)\n",
    "sh[1] = sh[1]\n",
    "binary_mask = torch.randint(0,2,sh)\n",
    "binary_mask = torch.concat((binary_mask, 1-binary_mask),dim=1)\n",
    "sh[1] = sh[1]*2\n",
    "\n",
    "input_ttfs = (torch.rand(sh)*(tmax - tmin) + tmin) * binary_mask\n",
    "input_x = (tmax - input_ttfs[0,:512]) - (tmax - input_ttfs[0,512:]) ### tu błąd masz poprawić że 0 m \n",
    "t_min__ = tmin\n",
    "t_max__ = tmax\n",
    "max_vect__ = torch.rand(1024)\n",
    "print(input_ttfs.shape)\n",
    "print(max_vect__.shape)\n",
    "print(input_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2302,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2 = SpikingDense_all_all(10,512, '',model2.fc.weight, model2.fc.bias,robustness_params=robustness_params)\n",
    "layer_fc = model2.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_19688\\2724487484.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "t_min, t_max, max_vect_temp = layer2.set_params(t_min__, t_max__, max_vect__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "tensor(0.0003, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out_tanh = layer2.forward(input_ttfs)\n",
    "out_x = model2.fc.forward(input_x)\n",
    "\n",
    "print(((t_max - out_tanh)[0,:10] - (t_max - out_tanh)[0,10:] - out_x).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(214.6736, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(t_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
