{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, end_maxpool = False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if(downsample is not None):\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(inplace=False),\n",
    "                            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                            )  # Changed inplace to False\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)\n",
    "                            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False))  # Changed inplace to False\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False)  # Changed inplace to False\n",
    "        self.out_channels = out_channels\n",
    "        self.end_maxpool = end_maxpool\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        if self.end_maxpool:\n",
    "            out = F.relu(out, inplace=False)\n",
    "        else:\n",
    "            out = F.hardtanh(out, inplace=False, min_val=-1.0, max_val=1.0)   # Use non-in-place ReLU\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2, in_channels = 5):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, 64, kernel_size = 7, stride = 1, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU(inplace=False))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2, end_maxpool = True)\n",
    "        self.avgpool = nn.MaxPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(2048, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, end_maxpool = False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, padding='same'),\n",
    "                nn.BatchNorm2d(planes),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            if i == blocks-1 and end_maxpool:\n",
    "                layers.append(block(self.inplanes, planes, end_maxpool = True))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.hardtanh(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nikos/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nikos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(12, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer0): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=1, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=1, bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=1, bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=1, padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "      )\n",
      "      (relu): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): MaxPool2d(kernel_size=7, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_resnet = ResNet(ResidualBlock, [3, 4, 6, 3], in_channels = 12)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "\n",
    "size_layer = [3, 4, 6, 3]\n",
    "layers = [(model_resnet.layer0, model.layer1), (model_resnet.layer1, model.layer2), (model_resnet.layer2, model.layer3), (model_resnet.layer3, model.layer4)]\n",
    "for n in range(len(layers)):\n",
    "    for i in range(size_layer[n]):\n",
    "        layers[n][0][i].conv1[0] = layers[n][1][i].conv1\n",
    "        layers[n][0][i].conv1[0].stride = 1\n",
    "        layers[n][0][i].conv2[0] = layers[n][1][i].conv2\n",
    "        layers[n][0][i].conv2[0].stride = 1\n",
    "        \n",
    "        layers[n][0][i].conv1[1] = layers[n][1][i].bn1\n",
    "        layers[n][0][i].conv2[1] = layers[n][1][i].bn2\n",
    "\n",
    "model_resnet.layer1[0].downsample[0] = model.layer2[0].downsample[0]\n",
    "model_resnet.layer1[0].downsample[0].stride = 1\n",
    "model_resnet.layer1[0].downsample[1] = model.layer2[0].downsample[1]\n",
    "\n",
    "model_resnet.layer2[0].downsample[0] = model.layer3[0].downsample[0]\n",
    "model_resnet.layer2[0].downsample[0].stride = 1\n",
    "model_resnet.layer2[0].downsample[1] = model.layer3[0].downsample[1]\n",
    "\n",
    "model_resnet.layer3[0].downsample[0] = model.layer4[0].downsample[0]\n",
    "model_resnet.layer3[0].downsample[0].stride = 1\n",
    "model_resnet.layer3[0].downsample[1] = model.layer4[0].downsample[1]\n",
    "\n",
    "model_resnet.to(\"cpu\")\n",
    "\n",
    "\n",
    "model_resnet.load_state_dict(torch.load(\"best_resnet_nCars_Hardtanh_ReLUmaxpool_EST_FC2__128x128_pretrained_aug_correct.pt\"))\n",
    "model_resnet.eval()\n",
    "print(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SNN.Check import fuse_conv_and_bn\n",
    "from SNN.Check import SpikingConv2D\n",
    "from SNN.Check import MaxMinPool2D\n",
    "from SNN.Check import LayerSNN_all\n",
    "from SNN.Check import SpikingDense_positive_tanH, SpikingDense_all_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResNet_ttfs(nn.Module):\n",
    "    def __init__(self, model : ResNet):\n",
    "        super(ResNet_ttfs, self).__init__()\n",
    "        model.eval()\n",
    "        robustness_params={\n",
    "            'noise':0.0,\n",
    "            'time_bits':0,\n",
    "            'weight_bits': 0,\n",
    "            'latency_quantiles':0.0\n",
    "        }\n",
    "        model.to('cpu')\n",
    "        conv_fused = fuse_conv_and_bn(model.conv1[0], model.conv1[1], device = 'cpu')\n",
    "        self.conv_first = SpikingConv2D(64, \"temp1\", device = 'cpu', padding=(3,3), stride=1, kernel_size=(7,7),robustness_params=robustness_params, kernels=conv_fused.weight.data, biases= conv_fused.bias.data)\n",
    "        max_vect = torch.tensor([1]*12)\n",
    "        tmin, tmax, max_vect, scalar = self.conv_first.set_params(0,1,max_vect)\n",
    "        self.tmin_post_pool = tmax\n",
    "        self.pool = MaxMinPool2D(3, tmax.data,2,1).to(\"cpu\")\n",
    "        self.layer0SNN = LayerSNN_all(model.layer0, 64, 64, 3,device = 'cpu')\n",
    "        tmax_prev = tmax\n",
    "        tmin, tmax, max_vect, scalar = self.layer0SNN.set_params(tmin, tmax, torch.concat((max_vect, torch.zeros(max_vect.shape))), in_scalar=scalar)\n",
    "        self.layer1SNN = LayerSNN_all(model.layer1, 64, 128, 4,device = 'cpu')\n",
    "        tmin, tmax, max_vect, scalar = self.layer1SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer2SNN = LayerSNN_all(model.layer2, 128, 256, 6,device = 'cpu')\n",
    "        tmin, tmax, max_vect, scalar = self.layer2SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer3SNN = LayerSNN_all(model.layer3, 256, 512, 3,device = 'cpu',end_maxpool=True)\n",
    "        tmin, tmax, max_vect, scalar = self.layer3SNN.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.pool2 = MaxMinPool2D(7, tmax.data,1,0).to(\"cpu\")\n",
    "        temp = torch.ones((512,2,2))\n",
    "        max_vect = ((temp.T)*max_vect[:512]).T\n",
    "        max_vect = max_vect.view(-1)\n",
    "        self.layer_fc = SpikingDense_positive_tanH(128,2048, '',model.fc.weight, model.fc.bias,robustness_params=robustness_params)\n",
    "        t_min, t_max, max_vect_temp, scalar = self.layer_fc.set_params(tmin, tmax, max_vect, in_scalar=scalar)\n",
    "        self.layer_fc2 = SpikingDense_all_all(2,128, '',model.fc2.weight, model.fc2.bias,robustness_params=robustness_params)\n",
    "        t_min, t_max, max_vect_temp, scalar = self.layer_fc2.set_params(t_min, t_max, max_vect_temp, in_scalar=scalar)\n",
    "        self.tmin, self.tmax = t_min, t_max\n",
    "        self.scalar = scalar\n",
    "    \n",
    "    def forward(self, tj):\n",
    "        x = self.conv_first(tj)\n",
    "        x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.concat((x, torch.ones(x.shape) * self.tmin_post_pool),dim=1)\n",
    "        x = self.layer0SNN(x)\n",
    "        x = self.layer1SNN(x)\n",
    "        x = self.layer2SNN(x)\n",
    "        x = self.layer3SNN(x)\n",
    "        # print(x[1].max(),x[1].min())\n",
    "        x = self.pool2(x[:512])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layer_fc(x)\n",
    "        x = self.layer_fc2(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:284: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:253: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:283: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:505: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:506: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:522: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:523: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:252: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:282: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:573: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:574: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:589: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:590: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:794: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:795: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:812: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:813: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:832: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:833: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n",
      "C:\\Users\\nikos\\AppData\\Local\\Temp\\ipykernel_29228\\2603052114.py:29: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  max_vect = ((temp.T)*max_vect[:512]).T\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:132: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(t_min + self.B_n*max_V, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_input = torch.concat((torch.tensor(in_ranges_max), torch.tensor([(1)])))\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:173: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min_prev = torch.tensor(t_min_prev, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_min = torch.tensor(t_min, dtype=torch.float64, requires_grad=False)\n",
      "c:\\D\\time_to_spike\\in_torch\\SNN\\Check.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.t_max = torch.tensor(max(t_min + self.B_n*max_V, minimal_t_max), dtype=torch.float64, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "model_ttfs = ResNet_ttfs(model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.3583, 5.3643, 5.3643, 5.3583]], grad_fn=<WhereBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.3687, 0.0000, 0.0000, 1.3687]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.rand((1,12,128,128))\n",
    "temp_ttfs = 1 - temp\n",
    "print(model_ttfs.forward(temp_ttfs))\n",
    "(model_ttfs.tmax - model_ttfs.forward(temp_ttfs))*model_ttfs.scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3686, -1.3686]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    # v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomPerspective(0.4),\n",
    "    v2.ToDtype(torch.float32)\n",
    "])\n",
    "\n",
    "class NCarsImageDataset(Dataset):\n",
    "    def __init__(self, img_dir_file, transform=None, target_transform=None):\n",
    "        self.images = np.load(img_dir_file + '_x.npy')\n",
    "        self.labels = np.load(img_dir_file + '_y.npy')\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        label_temp = np.zeros((2,))\n",
    "        label_temp[label] = 1\n",
    "        if self.stage == 0:\n",
    "            return self.transform(torch.tensor(image)), torch.tensor(label_temp)\n",
    "        else:\n",
    "            return torch.tensor(image), torch.tensor(label_temp)\n",
    "    \n",
    "    def set_stage(self, stage):\n",
    "        self.stage = stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# training_data = NCarsImageDataset(\"./Datasety/nCars_train_EST_exp_\", transform=transforms)\n",
    "test_data = NCarsImageDataset(\"./Datasety/nCars_test_EST_exp_\", transform=transforms)\n",
    "test_data.set_stage(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 128, 128])\n",
      "tensor([[5.3643, 5.3593, 5.3593, 5.3643]], grad_fn=<WhereBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.1571, 1.1571, 0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data, temp_label = test_data[10]\n",
    "temp_ttfs = (1 - temp_data).unsqueeze(0)\n",
    "print(temp_data.shape)\n",
    "print(model_ttfs.forward(temp_ttfs))\n",
    "(model_ttfs.tmax - model_ttfs.forward(temp_ttfs))*model_ttfs.scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1563,  1.1563]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet(temp_data.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
